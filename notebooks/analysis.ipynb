{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Detection - Analysis Notebook\n",
    "\n",
    "This notebook provides comprehensive analysis of the fake news detection system including:\n",
    "- Data exploration and visualization\n",
    "- Model training and comparison\n",
    "- Performance evaluation\n",
    "- Feature analysis\n",
    "- Example predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory to path to import our modules\n",
    "sys.path.append('../')\n",
    "from src.preprocess import NewsPreprocessor, create_sample_data\n",
    "from src.model import FakeNewsClassifier\n",
    "from src.predict import FakeNewsPredictor, quick_predict\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"üìö All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sample dataset\n",
    "df = pd.read_csv('../data/sample_dataset.csv')\n",
    "\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset statistics\n",
    "print(\"=== Dataset Statistics ===\")\n",
    "print(f\"Total articles: {len(df)}\")\n",
    "print(f\"Fake news articles: {sum(df['label'])} ({sum(df['label'])/len(df)*100:.1f}%)\")\n",
    "print(f\"Real news articles: {len(df) - sum(df['label'])} ({(len(df) - sum(df['label']))/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Text length analysis\n",
    "df['text_length'] = df['text'].str.len()\n",
    "df['word_count'] = df['text'].str.split().str.len()\n",
    "\n",
    "print(f\"\\nText Length Statistics:\")\n",
    "print(f\"Average text length: {df['text_length'].mean():.1f} characters\")\n",
    "print(f\"Average word count: {df['word_count'].mean():.1f} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Class distribution\n",
    "class_counts = df['label'].value_counts()\n",
    "axes[0, 0].pie(class_counts.values, labels=['Real News', 'Fake News'], autopct='%1.1f%%', startangle=90)\n",
    "axes[0, 0].set_title('Class Distribution')\n",
    "\n",
    "# Text length distribution\n",
    "axes[0, 1].hist(df[df['label']==0]['text_length'], alpha=0.7, label='Real News', bins=20)\n",
    "axes[0, 1].hist(df[df['label']==1]['text_length'], alpha=0.7, label='Fake News', bins=20)\n",
    "axes[0, 1].set_xlabel('Text Length (characters)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Text Length Distribution')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Word count distribution\n",
    "axes[1, 0].hist(df[df['label']==0]['word_count'], alpha=0.7, label='Real News', bins=20)\n",
    "axes[1, 0].hist(df[df['label']==1]['word_count'], alpha=0.7, label='Fake News', bins=20)\n",
    "axes[1, 0].set_xlabel('Word Count')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Word Count Distribution')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Box plot of text lengths by class\n",
    "df.boxplot(column='text_length', by='label', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Text Length by Class')\n",
    "axes[1, 1].set_xlabel('Class (0=Real, 1=Fake)')\n",
    "axes[1, 1].set_ylabel('Text Length')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = NewsPreprocessor(max_features=1000)\n",
    "\n",
    "# Example of text preprocessing\n",
    "sample_text = df.iloc[0]['text']\n",
    "print(\"=== Text Preprocessing Example ===\")\n",
    "print(f\"Original text: {sample_text}\")\n",
    "print(f\"Cleaned text: {preprocessor.clean_text(sample_text)}\")\n",
    "print(f\"Processed text: {preprocessor.preprocess_text(sample_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for analysis\n",
    "texts, labels = preprocessor.prepare_data(df)\n",
    "\n",
    "print(f\"Preprocessed texts: {len(texts)}\")\n",
    "print(f\"Labels: {len(labels)}\")\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\n=== Preprocessed Text Examples ===\")\n",
    "for i in range(3):\n",
    "    print(f\"Text {i+1} (Label: {labels[i]}): {texts[i][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.3, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "\n",
    "# Fit vectorizer and transform data\n",
    "preprocessor.fit_vectorizer(X_train)\n",
    "X_train_tfidf = preprocessor.transform_texts(X_train)\n",
    "X_test_tfidf = preprocessor.transform_texts(X_test)\n",
    "\n",
    "print(f\"Training features shape: {X_train_tfidf.shape}\")\n",
    "print(f\"Test features shape: {X_test_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models\n",
    "classifier = FakeNewsClassifier()\n",
    "print(\"üöÄ Training all models...\")\n",
    "results = classifier.train_all_models(X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "\n",
    "print(\"\\n‚úÖ Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison\n",
    "comparison_df = classifier.get_model_comparison()\n",
    "print(\"=== Model Performance Comparison ===\")\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model performance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# CV Scores\n",
    "axes[0, 0].bar(comparison_df['Model'], comparison_df['CV Mean'])\n",
    "axes[0, 0].set_title('Cross-Validation Scores')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Train vs Test Accuracy\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "axes[0, 1].bar(x - width/2, comparison_df['Train Accuracy'], width, label='Train', alpha=0.8)\n",
    "axes[0, 1].bar(x + width/2, comparison_df['Test Accuracy'], width, label='Test', alpha=0.8)\n",
    "axes[0, 1].set_title('Train vs Test Accuracy')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].set_xticks(x)\n",
    "axes[0, 1].set_xticklabels(comparison_df['Model'], rotation=45)\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# CV Score with Error Bars\n",
    "axes[1, 0].errorbar(comparison_df['Model'], comparison_df['CV Mean'], \n",
    "                   yerr=comparison_df['CV Std'], fmt='o', capsize=5, capthick=2, markersize=8)\n",
    "axes[1, 0].set_title('CV Scores with Standard Deviation')\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Performance heatmap\n",
    "performance_data = comparison_df[['Model', 'Train Accuracy', 'CV Mean', 'Test Accuracy']].set_index('Model')\n",
    "sns.heatmap(performance_data.T, annot=True, fmt='.3f', cmap='YlOrRd', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Performance Heatmap')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üèÜ Best Model: {classifier.best_model_name} (CV Score: {max(comparison_df['CV Mean']):.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Detailed Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of the best model\n",
    "best_model_name = classifier.best_model_name\n",
    "best_model_results = classifier.model_scores[best_model_name]\n",
    "\n",
    "print(f\"=== Detailed Analysis: {best_model_name.upper()} ===\")\n",
    "print(f\"Training Accuracy: {best_model_results['train_accuracy']:.4f}\")\n",
    "print(f\"Test Accuracy: {best_model_results['test_accuracy']:.4f}\")\n",
    "print(f\"CV Mean: {best_model_results['cv_mean']:.4f}\")\n",
    "print(f\"CV Std: {best_model_results['cv_std']:.4f}\")\n",
    "\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(best_model_results['classification_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Visualization\n",
    "cm = best_model_results['confusion_matrix']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Real News', 'Fake News'],\n",
    "            yticklabels=['Real News', 'Fake News'])\n",
    "plt.title(f'Confusion Matrix - {best_model_name.upper()}')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Calculate additional metrics\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(f\"\\n=== Additional Metrics ===\")\n",
    "print(f\"Precision (Fake News): {precision:.4f}\")\n",
    "print(f\"Recall (Fake News): {recall:.4f}\")\n",
    "print(f\"F1-Score (Fake News): {f1_score:.4f}\")\n",
    "print(f\"Specificity (Real News): {specificity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names and importance (for models that support it)\n",
    "feature_names = preprocessor.get_feature_names()\n",
    "print(f\"Total features: {len(feature_names)}\")\n",
    "print(f\"Sample features: {list(feature_names[:20])}\")\n",
    "\n",
    "# For linear models, we can analyze feature weights\n",
    "if 'logistic_regression' in classifier.trained_models:\n",
    "    lr_model = classifier.trained_models['logistic_regression']\n",
    "    coefficients = lr_model.coef_[0]\n",
    "    \n",
    "    # Get top positive and negative features\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'coefficient': coefficients\n",
    "    })\n",
    "    \n",
    "    top_fake = feature_importance.nlargest(10, 'coefficient')\n",
    "    top_real = feature_importance.nsmallest(10, 'coefficient')\n",
    "    \n",
    "    print(\"\\n=== Top Features Indicating FAKE NEWS ===\")\n",
    "    for _, row in top_fake.iterrows():\n",
    "        print(f\"{row['feature']}: {row['coefficient']:.4f}\")\n",
    "    \n",
    "    print(\"\\n=== Top Features Indicating REAL NEWS ===\")\n",
    "    for _, row in top_real.iterrows():\n",
    "        print(f\"{row['feature']}: {row['coefficient']:.4f}\")\n",
    "        \n",
    "    # Visualize feature importance\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Top fake news indicators\n",
    "    axes[0].barh(range(len(top_fake)), top_fake['coefficient'])\n",
    "    axes[0].set_yticks(range(len(top_fake)))\n",
    "    axes[0].set_yticklabels(top_fake['feature'])\n",
    "    axes[0].set_title('Top Features for Fake News Detection')\n",
    "    axes[0].set_xlabel('Coefficient Value')\n",
    "    \n",
    "    # Top real news indicators\n",
    "    axes[1].barh(range(len(top_real)), top_real['coefficient'])\n",
    "    axes[1].set_yticks(range(len(top_real)))\n",
    "    axes[1].set_yticklabels(top_real['feature'])\n",
    "    axes[1].set_title('Top Features for Real News Detection')\n",
    "    axes[1].set_xlabel('Coefficient Value')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prediction Examples and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with custom examples\n",
    "test_articles = [\n",
    "    \"Scientists at MIT have developed a new renewable energy technology that could revolutionize solar power.\",\n",
    "    \"BREAKING: Doctors HATE this one weird trick that cures everything! Click here to learn more!\",\n",
    "    \"The Federal Reserve announced today that interest rates will remain unchanged following their monthly meeting.\",\n",
    "    \"SHOCKING: Celebrity spotted with aliens, government cover-up exposed!\",\n",
    "    \"Local university receives federal grant for climate change research project.\"\n",
    "]\n",
    "\n",
    "print(\"=== Prediction Examples ===\")\n",
    "for i, article in enumerate(test_articles, 1):\n",
    "    # Make prediction using our trained model\n",
    "    processed_text = preprocessor.preprocess_text(article)\n",
    "    text_tfidf = preprocessor.transform_texts([processed_text])\n",
    "    prediction = classifier.predict(text_tfidf)[0]\n",
    "    \n",
    "    # Get probability if available\n",
    "    try:\n",
    "        probabilities = classifier.predict_proba(text_tfidf)[0]\n",
    "        confidence = max(probabilities)\n",
    "        fake_prob = probabilities[1]\n",
    "    except:\n",
    "        confidence = \"N/A\"\n",
    "        fake_prob = \"N/A\"\n",
    "    \n",
    "    label = \"FAKE NEWS\" if prediction == 1 else \"REAL NEWS\"\n",
    "    \n",
    "    print(f\"\\n--- Article {i} ---\")\n",
    "    print(f\"Text: {article}\")\n",
    "    print(f\"Prediction: {label}\")\n",
    "    if confidence != \"N/A\":\n",
    "        print(f\"Confidence: {confidence:.2%}\")\n",
    "        print(f\"Fake News Probability: {fake_prob:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Analysis - look at misclassified examples\n",
    "print(\"=== Error Analysis ===\")\n",
    "test_predictions = classifier.predict(X_test_tfidf)\n",
    "misclassified_indices = np.where(test_predictions != y_test)[0]\n",
    "\n",
    "print(f\"Total misclassified: {len(misclassified_indices)} out of {len(y_test)}\")\n",
    "print(f\"Error rate: {len(misclassified_indices)/len(y_test)*100:.1f}%\")\n",
    "\n",
    "# Show a few misclassified examples\n",
    "print(\"\\n=== Sample Misclassified Examples ===\")\n",
    "for i, idx in enumerate(misclassified_indices[:3]):\n",
    "    actual_label = \"FAKE NEWS\" if y_test[idx] == 1 else \"REAL NEWS\"\n",
    "    predicted_label = \"FAKE NEWS\" if test_predictions[idx] == 1 else \"REAL NEWS\"\n",
    "    \n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"Text: {X_test[idx]}\")\n",
    "    print(f\"Actual: {actual_label}\")\n",
    "    print(f\"Predicted: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Comparison Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive model comparison visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Individual model performance bars\n",
    "for i, model_name in enumerate(['naive_bayes', 'svm', 'random_forest', 'logistic_regression']):\n",
    "    row = i // 2\n",
    "    col = i % 2\n",
    "    \n",
    "    if model_name in classifier.model_scores:\n",
    "        scores = classifier.model_scores[model_name]\n",
    "        metrics = ['Train Acc', 'Test Acc', 'CV Mean']\n",
    "        values = [scores['train_accuracy'], scores['test_accuracy'], scores['cv_mean']]\n",
    "        \n",
    "        axes[row, col].bar(metrics, values, alpha=0.7)\n",
    "        axes[row, col].set_title(f'{model_name.upper()} Performance')\n",
    "        axes[row, col].set_ylim([0, 1])\n",
    "        axes[row, col].set_ylabel('Accuracy')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for j, v in enumerate(values):\n",
    "            axes[row, col].text(j, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Overall comparison\n",
    "models = comparison_df['Model']\n",
    "cv_scores = comparison_df['CV Mean']\n",
    "test_scores = comparison_df['Test Accuracy']\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "axes[0, 2].bar(x - width/2, cv_scores, width, label='CV Score', alpha=0.8)\n",
    "axes[0, 2].bar(x + width/2, test_scores, width, label='Test Score', alpha=0.8)\n",
    "axes[0, 2].set_title('Model Comparison')\n",
    "axes[0, 2].set_ylabel('Accuracy')\n",
    "axes[0, 2].set_xticks(x)\n",
    "axes[0, 2].set_xticklabels(models, rotation=45, ha='right')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].set_ylim([0, 1])\n",
    "\n",
    "# Model ranking\n",
    "sorted_df = comparison_df.sort_values('CV Mean', ascending=True)\n",
    "axes[1, 2].barh(range(len(sorted_df)), sorted_df['CV Mean'])\n",
    "axes[1, 2].set_yticks(range(len(sorted_df)))\n",
    "axes[1, 2].set_yticklabels(sorted_df['Model'])\n",
    "axes[1, 2].set_title('Model Ranking (by CV Score)')\n",
    "axes[1, 2].set_xlabel('CV Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final summary\n",
    "best_score = max(comparison_df['CV Mean'])\n",
    "best_test_score = comparison_df[comparison_df['Model'] == classifier.best_model_name]['Test Accuracy'].iloc[0]\n",
    "\n",
    "print(\"\" + \"=\"*60)\n",
    "print(\"üìä FAKE NEWS DETECTION - ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Dataset Size: {len(df)} articles\")\n",
    "print(f\"Feature Count: {len(feature_names)} TF-IDF features\")\n",
    "print(f\"Models Trained: {len(classifier.trained_models)}\")\n",
    "print(f\"\\nüèÜ Best Model: {classifier.best_model_name.upper()}\")\n",
    "print(f\"üéØ Cross-Validation Score: {best_score:.4f}\")\n",
    "print(f\"üéØ Test Accuracy: {best_test_score:.4f}\")\n",
    "print(f\"\\nüìà Model Rankings (by CV Score):\")\n",
    "for i, (_, row) in enumerate(comparison_df.sort_values('CV Mean', ascending=False).iterrows(), 1):\n",
    "    print(f\"{i}. {row['Model'].upper()}: {row['CV Mean']:.4f}\")\n",
    "\n",
    "print(f\"\\nüí° Recommendations:\")\n",
    "if best_score > 0.8:\n",
    "    print(\"‚úÖ Model performance is good for this dataset size\")\nelse:\n",
    "    print(\"‚ö†Ô∏è  Consider collecting more training data or feature engineering\")\n",
    "    \n",
    "print(\"üöÄ For production use:\")\n",
    "print(f\"   - Use the {classifier.best_model_name} model\")\n",
    "print(\"   - Consider ensemble methods for better performance\")\n",
    "print(\"   - Collect more diverse training data\")\n",
    "print(\"   - Implement confidence thresholds for predictions\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save analysis results\n",
    "import os\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "# Save model comparison\n",
    "comparison_df.to_csv('../results/model_comparison.csv', index=False)\n",
    "\n",
    "# Save feature analysis (if available)\n",
    "if 'feature_importance' in locals():\n",
    "    feature_importance.to_csv('../results/feature_importance.csv', index=False)\n",
    "\n",
    "# Save test predictions\n",
    "test_results = pd.DataFrame({\n",
    "    'text': X_test,\n",
    "    'actual_label': y_test,\n",
    "    'predicted_label': test_predictions,\n",
    "    'correct': y_test == test_predictions\n",
    "})\n",
    "test_results.to_csv('../results/test_predictions.csv', index=False)\n",
    "\n",
    "print(\"üìÅ Analysis results saved to ../results/ directory\")\n",
    "print(\"   - model_comparison.csv\")\n",
    "if 'feature_importance' in locals():\n",
    "    print(\"   - feature_importance.csv\")\n",
    "print(\"   - test_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This notebook has provided a comprehensive analysis of the fake news detection system, including:\n",
    "\n",
    "1. **Data Exploration**: Understanding the dataset characteristics and distributions\n",
    "2. **Preprocessing Analysis**: Examining the text cleaning and feature extraction process\n",
    "3. **Model Training**: Training and comparing multiple machine learning algorithms\n",
    "4. **Performance Evaluation**: Detailed analysis of model performance metrics\n",
    "5. **Feature Analysis**: Understanding which features are most important for classification\n",
    "6. **Prediction Examples**: Testing the system with real examples\n",
    "7. **Error Analysis**: Examining misclassified cases\n",
    "\n",
    "The system demonstrates good performance for fake news detection and can be further improved with more training data and advanced techniques.\n",
    "\n",
    "---\n",
    "\n",
    "*Run this notebook to explore your own fake news detection analysis!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}