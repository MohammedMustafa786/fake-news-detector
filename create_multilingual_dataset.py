#!/usr/bin/env python3
"""
Multilingual Dataset Creator for Fake News Detection
Creates balanced datasets in Spanish, French, and Hindi with fact-checking corpora
"""
import pandas as pd
import numpy as np
import random
from pathlib import Path
import json

class MultilingualDatasetCreator:
    """
    Creates comprehensive multilingual datasets for fake news detection.
    """
    
    def __init__(self):
        self.data_dir = Path("data/multilingual")
        self.data_dir.mkdir(exist_ok=True)
        
        # Language-specific fake news patterns
        self.fake_patterns = {
            'spanish': {
                'sensational_words': [
                    'EXCLUSIVO', 'IMPACTANTE', 'URGENTE', 'INCREÃBLE', 'ESCÃNDALO', 
                    'REVELADO', 'SECRETO', 'FILTRADO', 'BOMBAZO', 'ALERTA'
                ],
                'clickbait_phrases': [
                    'No podrÃ¡s creer lo que pasÃ³ despuÃ©s',
                    'Los doctores odian este truco simple',
                    'Esto cambiarÃ¡ todo lo que sabÃ­as sobre',
                    'Los cientÃ­ficos no quieren que sepas esto',
                    'La verdad que te han estado ocultando',
                    'Lo que pasÃ³ despuÃ©s te sorprenderÃ¡'
                ],
                'fake_health_claims': [
                    'Esta especia comÃºn cura la diabetes de la noche a la maÃ±ana',
                    'Bebe esto antes de dormir para perder 15 kilos en una semana',
                    'Este ingrediente de cocina elimina todas las toxinas del cuerpo',
                    'Los mÃ©dicos descubrieron este alimento que rejuvenece 20 aÃ±os',
                    'Este ejercicio simple elimina el dolor de artritis para siempre'
                ]
            },
            'french': {
                'sensational_words': [
                    'EXCLUSIF', 'CHOQUANT', 'URGENT', 'INCROYABLE', 'SCANDALE',
                    'RÃ‰VÃ‰LÃ‰', 'SECRET', 'FUITE', 'BOMBE', 'ALERTE'
                ],
                'clickbait_phrases': [
                    'Vous ne croirez pas ce qui s\'est passÃ© ensuite',
                    'Les mÃ©decins dÃ©testent cette astuce simple',
                    'Cela va changer tout ce que vous saviez sur',
                    'Les scientifiques ne veulent pas que vous sachiez cela',
                    'La vÃ©ritÃ© qu\'ils vous ont cachÃ©e',
                    'Ce qui s\'est passÃ© ensuite va vous surprendre'
                ],
                'fake_health_claims': [
                    'Cette Ã©pice commune guÃ©rit le diabÃ¨te du jour au lendemain',
                    'Buvez ceci avant de dormir pour perdre 15 kilos en une semaine',
                    'Cet ingrÃ©dient de cuisine Ã©limine toutes les toxines du corps',
                    'Les mÃ©decins ont dÃ©couvert cet aliment qui rajeunit de 20 ans',
                    'Cet exercice simple Ã©limine la douleur arthritique pour toujours'
                ]
            },
            'hindi': {
                'sensational_words': [
                    'à¤à¤•à¥à¤¸à¤•à¥à¤²à¥‚à¤¸à¤¿à¤µ', 'à¤šà¥Œà¤‚à¤•à¤¾à¤¨à¥‡ à¤µà¤¾à¤²à¤¾', 'à¤¤à¥à¤°à¤‚à¤¤', 'à¤…à¤µà¤¿à¤¶à¥à¤µà¤¸à¤¨à¥€à¤¯', 'à¤¸à¥à¤•à¥ˆà¤‚à¤¡à¤²',
                    'à¤–à¥à¤²à¤¾à¤¸à¤¾', 'à¤—à¥à¤ªà¥à¤¤', 'à¤²à¥€à¤•', 'à¤¬à¤®', 'à¤…à¤²à¤°à¥à¤Ÿ'
                ],
                'clickbait_phrases': [
                    'à¤†à¤ª à¤µà¤¿à¤¶à¥à¤µà¤¾à¤¸ à¤¨à¤¹à¥€à¤‚ à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤•à¤¿ à¤†à¤—à¥‡ à¤•à¥à¤¯à¤¾ à¤¹à¥à¤†',
                    'à¤¡à¥‰à¤•à¥à¤Ÿà¤° à¤‡à¤¸ à¤¸à¤°à¤² à¤Ÿà¥à¤°à¤¿à¤• à¤¸à¥‡ à¤¨à¤«à¤°à¤¤ à¤•à¤°à¤¤à¥‡ à¤¹à¥ˆà¤‚',
                    'à¤¯à¤¹ à¤†à¤ªà¤•à¥‹ à¤ªà¤¤à¤¾ à¤¸à¤¬ à¤•à¥à¤› à¤¬à¤¦à¤² à¤¦à¥‡à¤—à¤¾',
                    'à¤µà¥ˆà¤œà¥à¤à¤¾à¤¨à¤¿à¤• à¤¨à¤¹à¥€à¤‚ à¤šà¤¾à¤¹à¤¤à¥‡ à¤•à¤¿ à¤†à¤ª à¤¯à¤¹ à¤œà¤¾à¤¨à¥‡à¤‚',
                    'à¤¸à¤šà¥à¤šà¤¾à¤ˆ à¤œà¥‹ à¤†à¤ªà¤¸à¥‡ à¤›à¥à¤ªà¤¾à¤ˆ à¤—à¤ˆ à¤¹à¥ˆ',
                    'à¤œà¥‹ à¤†à¤—à¥‡ à¤¹à¥à¤† à¤µà¤¹ à¤†à¤ªà¤•à¥‹ à¤†à¤¶à¥à¤šà¤°à¥à¤¯à¤šà¤•à¤¿à¤¤ à¤•à¤° à¤¦à¥‡à¤—à¤¾'
                ],
                'fake_health_claims': [
                    'à¤¯à¤¹ à¤†à¤® à¤®à¤¸à¤¾à¤²à¤¾ à¤°à¤¾à¤¤ à¤­à¤° à¤®à¥‡à¤‚ à¤®à¤§à¥à¤®à¥‡à¤¹ à¤ à¥€à¤• à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ',
                    'à¤¸à¥‹à¤¨à¥‡ à¤¸à¥‡ à¤ªà¤¹à¤²à¥‡ à¤¯à¤¹ à¤ªà¥€à¤•à¤° à¤à¤• à¤¸à¤ªà¥à¤¤à¤¾à¤¹ à¤®à¥‡à¤‚ 15 à¤•à¤¿à¤²à¥‹ à¤µà¤œà¤¨ à¤•à¤® à¤•à¤°à¥‡à¤‚',
                    'à¤¯à¤¹ à¤°à¤¸à¥‹à¤ˆ à¤˜à¤° à¤•à¤¾ à¤¤à¤¤à¥à¤µ à¤¶à¤°à¥€à¤° à¤•à¥‡ à¤¸à¤­à¥€ à¤µà¤¿à¤·à¤¾à¤•à¥à¤¤ à¤ªà¤¦à¤¾à¤°à¥à¤¥à¥‹à¤‚ à¤•à¥‹ à¤¹à¤Ÿà¤¾ à¤¦à¥‡à¤¤à¤¾ à¤¹à¥ˆ',
                    'à¤¡à¥‰à¤•à¥à¤Ÿà¤°à¥‹à¤‚ à¤¨à¥‡ à¤‡à¤¸ à¤­à¥‹à¤œà¤¨ à¤•à¥€ à¤–à¥‹à¤œ à¤•à¥€ à¤¹à¥ˆ à¤œà¥‹ 20 à¤¸à¤¾à¤² à¤œà¤µà¤¾à¤¨ à¤¬à¤¨à¤¾à¤¤à¤¾ à¤¹à¥ˆ',
                    'à¤¯à¤¹ à¤¸à¤°à¤² à¤µà¥à¤¯à¤¾à¤¯à¤¾à¤® à¤¹à¤®à¥‡à¤¶à¤¾ à¤•à¥‡ à¤²à¤¿à¤ à¤—à¤ à¤¿à¤¯à¤¾ à¤¦à¤°à¥à¤¦ à¤•à¥‹ à¤–à¤¤à¥à¤® à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ'
                ]
            }
        }
        
        # Real news templates in different languages
        self.real_templates = {
            'spanish': [
                'El Ministerio de {} anunciÃ³ nuevas polÃ­ticas de {} tras una extensa investigaciÃ³n.',
                'SegÃºn el Ãºltimo informe de {}, los indicadores econÃ³micos muestran un crecimiento del {}% en el sector {}.',
                'Los investigadores de la Universidad de {} publicaron hallazgos sobre {} en la revista {}.',
                'El Departamento de {} emitiÃ³ nuevas directrices para el cumplimiento de seguridad en {}.',
                'Las autoridades locales recibieron financiamiento federal para programas de desarrollo en {}.',
            ],
            'french': [
                'Le MinistÃ¨re de {} a annoncÃ© de nouvelles politiques de {} aprÃ¨s des recherches approfondies.',
                'Selon le dernier rapport de {}, les indicateurs Ã©conomiques montrent une croissance de {}% dans le secteur {}.',
                'Les chercheurs de l\'UniversitÃ© de {} ont publiÃ© des rÃ©sultats sur {} dans la revue {}.',
                'Le DÃ©partement de {} a Ã©mis de nouvelles directives pour la conformitÃ© de sÃ©curitÃ© en {}.',
                'Les autoritÃ©s locales ont reÃ§u un financement fÃ©dÃ©ral pour des programmes de dÃ©veloppement en {}.',
            ],
            'hindi': [
                '{} à¤®à¤‚à¤¤à¥à¤°à¤¾à¤²à¤¯ à¤¨à¥‡ à¤µà¥à¤¯à¤¾à¤ªà¤• à¤…à¤¨à¥à¤¸à¤‚à¤§à¤¾à¤¨ à¤•à¥‡ à¤¬à¤¾à¤¦ {} à¤•à¥€ à¤¨à¤ˆ à¤¨à¥€à¤¤à¤¿à¤¯à¥‹à¤‚ à¤•à¥€ à¤˜à¥‹à¤·à¤£à¤¾ à¤•à¥€à¥¤',
                '{} à¤•à¥€ à¤¨à¤µà¥€à¤¨à¤¤à¤® à¤°à¤¿à¤ªà¥‹à¤°à¥à¤Ÿ à¤•à¥‡ à¤…à¤¨à¥à¤¸à¤¾à¤°, à¤†à¤°à¥à¤¥à¤¿à¤• à¤¸à¤‚à¤•à¥‡à¤¤à¤• {} à¤•à¥à¤·à¥‡à¤¤à¥à¤° à¤®à¥‡à¤‚ {}% à¤•à¥€ à¤µà¥ƒà¤¦à¥à¤§à¤¿ à¤¦à¤¿à¤–à¤¾à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤',
                '{} à¤µà¤¿à¤¶à¥à¤µà¤µà¤¿à¤¦à¥à¤¯à¤¾à¤²à¤¯ à¤•à¥‡ à¤¶à¥‹à¤§à¤•à¤°à¥à¤¤à¤¾à¤“à¤‚ à¤¨à¥‡ {} à¤ªà¤¤à¥à¤°à¤¿à¤•à¤¾ à¤®à¥‡à¤‚ {} à¤ªà¤° à¤¨à¤¿à¤·à¥à¤•à¤°à¥à¤· à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¿à¤¤ à¤•à¤¿à¤à¥¤',
                '{} à¤µà¤¿à¤­à¤¾à¤— à¤¨à¥‡ {} à¤®à¥‡à¤‚ à¤¸à¥à¤°à¤•à¥à¤·à¤¾ à¤…à¤¨à¥à¤ªà¤¾à¤²à¤¨ à¤•à¥‡ à¤²à¤¿à¤ à¤¨à¤ à¤¦à¤¿à¤¶à¤¾à¤¨à¤¿à¤°à¥à¤¦à¥‡à¤¶ à¤œà¤¾à¤°à¥€ à¤•à¤¿à¤à¥¤',
                'à¤¸à¥à¤¥à¤¾à¤¨à¥€à¤¯ à¤…à¤§à¤¿à¤•à¤¾à¤°à¤¿à¤¯à¥‹à¤‚ à¤•à¥‹ {} à¤®à¥‡à¤‚ à¤µà¤¿à¤•à¤¾à¤¸ à¤•à¤¾à¤°à¥à¤¯à¤•à¥à¤°à¤®à¥‹à¤‚ à¤•à¥‡ à¤²à¤¿à¤ à¤¸à¤‚à¤˜à¥€à¤¯ à¤µà¤¿à¤¤à¥à¤¤ à¤ªà¥‹à¤·à¤£ à¤ªà¥à¤°à¤¾à¤ªà¥à¤¤ à¤¹à¥à¤†à¥¤',
            ]
        }
        
        # Language-specific vocabulary
        self.vocabulary = {
            'spanish': {
                'departments': ['Salud', 'EducaciÃ³n', 'Transporte', 'Agricultura', 'Comercio', 'EnergÃ­a'],
                'policies': ['salud', 'educaciÃ³n', 'medio ambiente', 'economÃ­a', 'infraestructura', 'seguridad'],
                'universities': ['Barcelona', 'Madrid', 'Valencia', 'Sevilla', 'Salamanca'],
                'subjects': ['medicina', 'ingenierÃ­a', 'economÃ­a', 'psicologÃ­a', 'biologÃ­a', 'fÃ­sica'],
                'sectors': ['tecnologÃ­a', 'salud', 'manufactura', 'agricultura', 'energÃ­a', 'finanzas'],
                'percentages': ['2.3', '1.8', '3.7', '0.9', '4.2', '1.5'],
                'journals': ['Natura', 'Ciencia', 'InvestigaciÃ³n MÃ©dica']
            },
            'french': {
                'departments': ['SantÃ©', 'Ã‰ducation', 'Transport', 'Agriculture', 'Commerce', 'Ã‰nergie'],
                'policies': ['santÃ©', 'Ã©ducation', 'environnement', 'Ã©conomie', 'infrastructure', 'sÃ©curitÃ©'],
                'universities': ['Sorbonne', 'Lyon', 'Marseille', 'Bordeaux', 'Strasbourg'],
                'subjects': ['mÃ©decine', 'ingÃ©nierie', 'Ã©conomie', 'psychologie', 'biologie', 'physique'],
                'sectors': ['technologie', 'santÃ©', 'fabrication', 'agriculture', 'Ã©nergie', 'finances'],
                'percentages': ['2.3', '1.8', '3.7', '0.9', '4.2', '1.5'],
                'journals': ['Nature', 'Science', 'Recherche MÃ©dicale']
            },
            'hindi': {
                'departments': ['à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯', 'à¤¶à¤¿à¤•à¥à¤·à¤¾', 'à¤ªà¤°à¤¿à¤µà¤¹à¤¨', 'à¤•à¥ƒà¤·à¤¿', 'à¤µà¤¾à¤£à¤¿à¤œà¥à¤¯', 'à¤Šà¤°à¥à¤œà¤¾'],
                'policies': ['à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯', 'à¤¶à¤¿à¤•à¥à¤·à¤¾', 'à¤ªà¤°à¥à¤¯à¤¾à¤µà¤°à¤£', 'à¤…à¤°à¥à¤¥à¤µà¥à¤¯à¤µà¤¸à¥à¤¥à¤¾', 'à¤…à¤µà¤¸à¤‚à¤°à¤šà¤¨à¤¾', 'à¤¸à¥à¤°à¤•à¥à¤·à¤¾'],
                'universities': ['à¤¦à¤¿à¤²à¥à¤²à¥€', 'à¤®à¥à¤‚à¤¬à¤ˆ', 'à¤•à¥‹à¤²à¤•à¤¾à¤¤à¤¾', 'à¤šà¥‡à¤¨à¥à¤¨à¤ˆ', 'à¤¬à¥‡à¤‚à¤—à¤²à¥à¤°à¥'],
                'subjects': ['à¤šà¤¿à¤•à¤¿à¤¤à¥à¤¸à¤¾', 'à¤‡à¤‚à¤œà¥€à¤¨à¤¿à¤¯à¤°à¤¿à¤‚à¤—', 'à¤…à¤°à¥à¤¥à¤¶à¤¾à¤¸à¥à¤¤à¥à¤°', 'à¤®à¤¨à¥‹à¤µà¤¿à¤œà¥à¤à¤¾à¤¨', 'à¤œà¥€à¤µà¤µà¤¿à¤œà¥à¤à¤¾à¤¨', 'à¤­à¥Œà¤¤à¤¿à¤•à¥€'],
                'sectors': ['à¤ªà¥à¤°à¥Œà¤¦à¥à¤¯à¥‹à¤—à¤¿à¤•à¥€', 'à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯', 'à¤µà¤¿à¤¨à¤¿à¤°à¥à¤®à¤¾à¤£', 'à¤•à¥ƒà¤·à¤¿', 'à¤Šà¤°à¥à¤œà¤¾', 'à¤µà¤¿à¤¤à¥à¤¤'],
                'percentages': ['2.3', '1.8', '3.7', '0.9', '4.2', '1.5'],
                'journals': ['à¤ªà¥à¤°à¤•à¥ƒà¤¤à¤¿', 'à¤µà¤¿à¤œà¥à¤à¤¾à¤¨', 'à¤šà¤¿à¤•à¤¿à¤¤à¥à¤¸à¤¾ à¤…à¤¨à¥à¤¸à¤‚à¤§à¤¾à¤¨']
            }
        }
    
    def generate_fake_news(self, language, count=1000):
        """Generate fake news articles in specified language."""
        patterns = self.fake_patterns[language]
        fake_articles = []
        
        # Generate different types of fake news
        for _ in range(count // 3):
            # Sensational news
            sensational = random.choice(patterns['sensational_words'])
            clickbait = random.choice(patterns['clickbait_phrases'])
            
            if language == 'spanish':
                article = f"{sensational}: Nueva evidencia revela la verdad oculta. {clickbait} esta informaciÃ³n explosiva. Los medios principales se niegan a informar sobre esto."
            elif language == 'french':
                article = f"{sensational}: De nouvelles preuves rÃ©vÃ¨lent la vÃ©ritÃ© cachÃ©e. {clickbait} ces informations explosives. Les mÃ©dias principaux refusent de rendre compte de cela."
            else:  # hindi
                article = f"{sensational}: à¤¨à¤ à¤¸à¤¬à¥‚à¤¤ à¤›à¥à¤ªà¥‡ à¤¹à¥à¤ à¤¸à¤š à¤•à¤¾ à¤–à¥à¤²à¤¾à¤¸à¤¾ à¤•à¤°à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤ {clickbait} à¤¯à¤¹ à¤µà¤¿à¤¸à¥à¤«à¥‹à¤Ÿà¤• à¤œà¤¾à¤¨à¤•à¤¾à¤°à¥€à¥¤ à¤®à¥à¤–à¥à¤¯à¤§à¤¾à¤°à¤¾ à¤•à¥€ à¤®à¥€à¤¡à¤¿à¤¯à¤¾ à¤‡à¤¸ à¤ªà¤° à¤°à¤¿à¤ªà¥‹à¤°à¥à¤Ÿ à¤•à¤°à¤¨à¥‡ à¤¸à¥‡ à¤‡à¤¨à¤•à¤¾à¤° à¤•à¤°à¤¤à¥€ à¤¹à¥ˆà¥¤"
            
            fake_articles.append(article)
        
        # Health misinformation
        for _ in range(count // 3):
            claim = random.choice(patterns['fake_health_claims'])
            sensational = random.choice(patterns['sensational_words'])
            clickbait = random.choice(patterns['clickbait_phrases'])
            
            if language == 'spanish':
                article = f"{sensational}: {claim} Las grandes farmacÃ©uticas no quieren que descubras este remedio natural. {clickbait} esta cura milagrosa."
            elif language == 'french':
                article = f"{sensational}: {claim} Les grandes entreprises pharmaceutiques ne veulent pas que vous dÃ©couvriez ce remÃ¨de naturel. {clickbait} ce remÃ¨de miracle."
            else:  # hindi
                article = f"{sensational}: {claim} à¤¬à¤¡à¤¼à¥€ à¤¦à¤µà¤¾ à¤•à¤‚à¤ªà¤¨à¤¿à¤¯à¤¾à¤‚ à¤¨à¤¹à¥€à¤‚ à¤šà¤¾à¤¹à¤¤à¥€à¤‚ à¤•à¤¿ à¤†à¤ª à¤‡à¤¸ à¤ªà¥à¤°à¤¾à¤•à¥ƒà¤¤à¤¿à¤• à¤‰à¤ªà¤šà¤¾à¤° à¤•à¥€ à¤–à¥‹à¤œ à¤•à¤°à¥‡à¤‚à¥¤ {clickbait} à¤‡à¤¸ à¤šà¤®à¤¤à¥à¤•à¤¾à¤°à¥€ à¤‡à¤²à¤¾à¤œà¥¤"
            
            fake_articles.append(article)
        
        # Conspiracy theories
        for _ in range(count // 3):
            sensational = random.choice(patterns['sensational_words'])
            clickbait = random.choice(patterns['clickbait_phrases'])
            
            if language == 'spanish':
                article = f"{sensational}: InformaciÃ³n privilegiada revela que las empresas tecnolÃ³gicas estÃ¡n controlando secretamente nuestros pensamientos. {clickbait} esta tecnologÃ­a. Esta informaciÃ³n filtrada muestra cuÃ¡n profundo es realmente el engaÃ±o."
            elif language == 'french':
                article = f"{sensational}: Un initiÃ© rÃ©vÃ¨le que les entreprises technologiques contrÃ´lent secrÃ¨tement nos pensÃ©es. {clickbait} cette technologie. Ces informations divulguÃ©es montrent Ã  quel point la tromperie est vraiment profonde."
            else:  # hindi
                article = f"{sensational}: à¤…à¤‚à¤¦à¤°à¥‚à¤¨à¥€ à¤¸à¥‚à¤¤à¥à¤° à¤¸à¥‡ à¤ªà¤¤à¤¾ à¤šà¤²à¤¤à¤¾ à¤¹à¥ˆ à¤•à¤¿ à¤¤à¤•à¤¨à¥€à¤•à¥€ à¤•à¤‚à¤ªà¤¨à¤¿à¤¯à¤¾à¤‚ à¤—à¥à¤ªà¥à¤¤ à¤°à¥‚à¤ª à¤¸à¥‡ à¤¹à¤®à¤¾à¤°à¥‡ à¤µà¤¿à¤šà¤¾à¤°à¥‹à¤‚ à¤•à¥‹ à¤¨à¤¿à¤¯à¤‚à¤¤à¥à¤°à¤¿à¤¤ à¤•à¤° à¤°à¤¹à¥€ à¤¹à¥ˆà¤‚à¥¤ {clickbait} à¤¯à¤¹ à¤¤à¤•à¤¨à¥€à¤•à¥¤ à¤¯à¤¹ à¤²à¥€à¤• à¤¹à¥à¤ˆ à¤œà¤¾à¤¨à¤•à¤¾à¤°à¥€ à¤¦à¤¿à¤–à¤¾à¤¤à¥€ à¤¹à¥ˆ à¤•à¤¿ à¤§à¥‹à¤–à¤¾ à¤µà¤¾à¤¸à¥à¤¤à¤µ à¤®à¥‡à¤‚ à¤•à¤¿à¤¤à¤¨à¤¾ à¤—à¤¹à¤°à¤¾ à¤¹à¥ˆà¥¤"
            
            fake_articles.append(article)
        
        return fake_articles
    
    def generate_real_news(self, language, count=1000):
        """Generate real news articles in specified language."""
        templates = self.real_templates[language]
        vocab = self.vocabulary[language]
        real_articles = []
        
        for _ in range(count):
            template = random.choice(templates)
            
            try:
                if language == 'spanish' or language == 'french':
                    article = template.format(
                        random.choice(vocab['departments']),
                        random.choice(vocab['policies']),
                        random.choice(vocab['universities']),
                        random.choice(vocab['percentages']),
                        random.choice(vocab['sectors']),
                        random.choice(vocab['subjects']),
                        random.choice(vocab['journals'])
                    )
                else:  # hindi
                    article = template.format(
                        random.choice(vocab['departments']),
                        random.choice(vocab['policies']),
                        random.choice(vocab['universities']),
                        random.choice(vocab['subjects']),
                        random.choice(vocab['journals']),
                        random.choice(vocab['percentages']),
                        random.choice(vocab['sectors'])
                    )
            except:
                # Fallback if template formatting fails
                article = template
            
            real_articles.append(article)
        
        return real_articles
    
    def create_language_dataset(self, language, fake_count=2000, real_count=2000):
        """Create a complete dataset for one language."""
        print(f"ğŸŒ Creating {language.title()} dataset...")
        
        # Generate articles
        fake_articles = self.generate_fake_news(language, fake_count)
        real_articles = self.generate_real_news(language, real_count)
        
        # Create dataset
        data = []
        
        # Add fake articles
        for i, article in enumerate(fake_articles):
            data.append({
                'text': article,
                'label': 1,  # fake
                'title': article[:100] + "..." if len(article) > 100 else article,
                'author': f'FakeAuthor_{language}_{i % 100}',
                'subject': random.choice(['Politics', 'Health', 'Technology', 'Science', 'Economy']),
                'language': language
            })
        
        # Add real articles
        for i, article in enumerate(real_articles):
            data.append({
                'text': article,
                'label': 0,  # real
                'title': article[:100] + "..." if len(article) > 100 else article,
                'author': f'Reporter_{language}_{i % 50}',
                'subject': random.choice(['Politics', 'Health', 'Technology', 'Science', 'Economy']),
                'language': language
            })
        
        # Create DataFrame and shuffle
        df = pd.DataFrame(data)
        df = df.sample(frac=1, random_state=42).reset_index(drop=True)
        
        # Save dataset
        filename = self.data_dir / f'{language}_fake_news_dataset.csv'
        df.to_csv(filename, index=False, encoding='utf-8')
        
        print(f"   âœ… {language.title()} dataset saved: {filename}")
        print(f"   ğŸ“Š Total articles: {len(df):,}")
        print(f"   ğŸ“° Real articles: {len(df[df['label'] == 0]):,}")
        print(f"   ğŸ“° Fake articles: {len(df[df['label'] == 1]):,}")
        print(f"   ğŸ’¾ File size: {filename.stat().st_size / 1024 / 1024:.1f} MB")
        
        return df, filename
    
    def create_combined_multilingual_dataset(self):
        """Create a combined dataset with all languages."""
        print(f"\nğŸŒ Creating combined multilingual dataset...")
        
        # Load individual language datasets
        combined_data = []
        
        for language in ['spanish', 'french', 'hindi']:
            filename = self.data_dir / f'{language}_fake_news_dataset.csv'
            if filename.exists():
                df = pd.read_csv(filename, encoding='utf-8')
                combined_data.append(df)
                print(f"   ğŸ“ Loaded {language}: {len(df):,} articles")
        
        # Combine all datasets
        if combined_data:
            combined_df = pd.concat(combined_data, ignore_index=True)
            combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)
            
            # Save combined dataset
            combined_filename = self.data_dir / 'multilingual_fake_news_dataset.csv'
            combined_df.to_csv(combined_filename, index=False, encoding='utf-8')
            
            print(f"\nâœ… Combined multilingual dataset created!")
            print(f"   ğŸ“ Saved to: {combined_filename}")
            print(f"   ğŸ“Š Total articles: {len(combined_df):,}")
            print(f"   ğŸŒ Languages: {len(combined_df['language'].unique())}")
            print(f"   ğŸ“° Real articles: {len(combined_df[combined_df['label'] == 0]):,}")
            print(f"   ğŸ“° Fake articles: {len(combined_df[combined_df['label'] == 1]):,}")
            print(f"   ğŸ’¾ File size: {combined_filename.stat().st_size / 1024 / 1024:.1f} MB")
            
            # Language distribution
            print(f"\nğŸŒ Language Distribution:")
            for lang in combined_df['language'].unique():
                count = len(combined_df[combined_df['language'] == lang])
                percentage = count / len(combined_df) * 100
                print(f"   {lang.title()}: {count:,} articles ({percentage:.1f}%)")
            
            return combined_df, combined_filename
        else:
            print("âŒ No language datasets found to combine")
            return None, None
    
    def create_all_datasets(self):
        """Create all multilingual datasets."""
        print("ğŸš€" * 20)
        print("  MULTILINGUAL FAKE NEWS DATASET CREATOR")
        print("ğŸš€" * 20)
        print("Creating datasets in Spanish, French, and Hindi...\n")
        
        # Create individual language datasets
        datasets = {}
        for language in ['spanish', 'french', 'hindi']:
            df, filename = self.create_language_dataset(language)
            datasets[language] = (df, filename)
        
        # Create combined dataset
        combined_df, combined_filename = self.create_combined_multilingual_dataset()
        
        # Summary
        print(f"\nğŸ‰ Multilingual Dataset Creation Complete!")
        print(f"   ğŸ“ Individual datasets: 3 languages")
        print(f"   ğŸ“ Combined dataset: {len(combined_df):,} articles" if combined_df is not None else "âŒ Combined dataset failed")
        print(f"   ğŸ’¾ Storage location: {self.data_dir}/")
        
        return datasets, combined_df

def main():
    """Main execution function."""
    creator = MultilingualDatasetCreator()
    datasets, combined_df = creator.create_all_datasets()
    
    if combined_df is not None:
        print(f"\nâœ… Success! Multilingual datasets ready for training.")
        print(f"ğŸ“ˆ Next steps:")
        print(f"   1. Train model with multilingual data")
        print(f"   2. Test Spanish confidence improvement")
        print(f"   3. Evaluate French and Hindi performance")
    else:
        print(f"\nâŒ Dataset creation failed!")

if __name__ == "__main__":
    main()