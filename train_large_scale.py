#!/usr/bin/env python3
"""
Large-scale training script for fake news detection.
Optimized for datasets with 40k+ articles using advanced preprocessing and model training.
"""
import os
import sys
import pandas as pd
import numpy as np
import argparse
from datetime import datetime
import warnings
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import joblib
from tqdm import tqdm
import gc

warnings.filterwarnings('ignore')

# Add src directory to Python path
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from src.large_scale_preprocess import LargeScaleNewsPreprocessor, load_large_dataset
from src.model import FakeNewsClassifier

class LargeScaleTrainer:
    """
    High-performance trainer for large-scale fake news detection.
    """
    
    def __init__(self, max_features=20000, batch_size=1000, use_lemmatization=True):
        self.max_features = max_features\n        self.batch_size = batch_size\n        self.use_lemmatization = use_lemmatization\n        \n        # Initialize components\n        self.preprocessor = LargeScaleNewsPreprocessor(\n            max_features=max_features,\n            use_lemmatization=use_lemmatization,\n            batch_size=batch_size\n        )\n        \n        self.classifier = FakeNewsClassifier()\n        \n        # Performance tracking\n        self.training_history = {}\n        self.best_scores = {}\n    \n    def load_and_prepare_data(self, data_path, sample_size=None):\n        \"\"\"\n        Load and prepare the dataset for training.\n        \n        Args:\n            data_path (str): Path to the dataset\n            sample_size (int): Optional sample size for testing (None = full dataset)\n            \n        Returns:\n            tuple: Training and testing data\n        \"\"\"\n        print(f\"\\n\ud83d\udcc1 Loading dataset from {data_path}...\")\n        \n        # Load dataset\n        df = load_large_dataset(data_path)\n        \n        if df is None:\n            raise FileNotFoundError(f\"Could not load dataset from {data_path}\")\n        \n        # Sample data if specified (for testing)\n        if sample_size and sample_size < len(df):\n            print(f\"\\n\ud83c\udfaf Sampling {sample_size:,} articles for training...\")\n            df = df.sample(n=sample_size, random_state=42, stratify=df['label'])\n        \n        # Check class distribution\n        real_count = len(df[df['label'] == 0])\n        fake_count = len(df[df['label'] == 1])\n        \n        print(f\"\\n\ud83d\udcca Dataset statistics:\")\n        print(f\"   Total articles: {len(df):,}\")\n        print(f\"   Real news: {real_count:,} ({real_count/len(df)*100:.1f}%)\")\n        print(f\"   Fake news: {fake_count:,} ({fake_count/len(df)*100:.1f}%)\")\n        print(f\"   Balance ratio: {min(real_count, fake_count) / max(real_count, fake_count):.3f}\")\n        \n        # Prepare data\n        print(f\"\\n\ud83d\udd04 Starting preprocessing pipeline...\")\n        texts, labels = self.preprocessor.prepare_data_large_scale(df)\n        \n        # Split data\n        print(f\"\\n\ud83d\udd04 Splitting data (80% train, 20% test)...\")\n        X_train, X_test, y_train, y_test = train_test_split(\n            texts, labels, \n            test_size=0.2, \n            random_state=42, \n            stratify=labels\n        )\n        \n        print(f\"   Training set: {len(X_train):,} samples\")\n        print(f\"   Testing set: {len(X_test):,} samples\")\n        \n        return X_train, X_test, y_train, y_test\n    \n    def fit_vectorizer_and_transform(self, X_train, X_test):\n        \"\"\"\n        Fit vectorizer on training data and transform both sets.\n        \n        Args:\n            X_train (list): Training texts\n            X_test (list): Testing texts\n            \n        Returns:\n            tuple: Transformed training and testing matrices\n        \"\"\"\n        print(f\"\\n\ud83d\udd27 Fitting TF-IDF vectorizer...\")\n        \n        # Fit vectorizer on training data\n        self.preprocessor.fit_vectorizer_large_scale(X_train)\n        \n        # Transform training data\n        print(f\"\\n\ud83d\udd04 Transforming training data...\")\n        X_train_tfidf = self.preprocessor.transform_texts_large_scale(X_train)\n        \n        # Transform testing data\n        print(f\"\\n\ud83d\udd04 Transforming testing data...\")\n        X_test_tfidf = self.preprocessor.transform_texts_large_scale(X_test)\n        \n        # Memory cleanup\n        del X_train, X_test\n        gc.collect()\n        \n        return X_train_tfidf, X_test_tfidf\n    \n    def train_models(self, X_train_tfidf, y_train, X_test_tfidf, y_test):\n        \"\"\"\n        Train all available models and evaluate performance.\n        \n        Args:\n            X_train_tfidf: Training TF-IDF matrix\n            y_train: Training labels\n            X_test_tfidf: Testing TF-IDF matrix\n            y_test: Testing labels\n            \n        Returns:\n            dict: Training results for all models\n        \"\"\"\n        print(f\"\\n\ud83c\udf86 Training all models on large dataset...\")\n        print(\"=\" * 60)\n        \n        # Train all models\n        results = self.classifier.train_all_models(\n            X_train_tfidf, y_train, X_test_tfidf, y_test\n        )\n        \n        # Enhanced evaluation\n        print(f\"\\n\ud83d\udcc8 Enhanced Model Evaluation:\")\n        print(\"=\" * 60)\n        \n        enhanced_results = {}\n        \n        for model_name, model_results in results.items():\n            print(f\"\\n\ud83e\udd16 {model_name.upper()} Results:\")\n            \n            # Get predictions for detailed metrics\n            model = self.classifier.trained_models[model_name]\n            y_pred = model.predict(X_test_tfidf)\n            \n            # Calculate additional metrics\n            try:\n                y_pred_proba = model.predict_proba(X_test_tfidf)[:, 1]\n                auc_score = roc_auc_score(y_test, y_pred_proba)\n            except:\n                auc_score = None\n            \n            # Store enhanced results\n            enhanced_results[model_name] = {\n                **model_results,\n                'auc_score': auc_score,\n                'predictions': y_pred\n            }\n            \n            # Print metrics\n            print(f\"   Training Accuracy: {model_results['train_accuracy']:.4f}\")\n            print(f\"   CV Score: {model_results['cv_mean']:.4f} (Â±{model_results['cv_std']:.4f})\")\n            print(f\"   Test Accuracy: {model_results['test_accuracy']:.4f}\")\n            if auc_score:\n                print(f\"   AUC Score: {auc_score:.4f}\")\n            \n            # Classification report\n            print(f\"\\n   Classification Report:\")\n            print(classification_report(y_test, y_pred, target_names=['Real', 'Fake'], \n                                     labels=[0, 1], digits=4))\n            \n            print(\"-\" * 50)\n        \n        # Update training history\n        self.training_history = enhanced_results\n        \n        return enhanced_results\n    \n    def save_models_and_results(self, model_dir='models_large_scale'):\n        \"\"\"\n        Save all trained models, vectorizer, and results.\n        \n        Args:\n            model_dir (str): Directory to save models\n        \"\"\"\n        os.makedirs(model_dir, exist_ok=True)\n        \n        print(f\"\\n\ud83d\udcbe Saving models and results to {model_dir}/...\")\n        \n        # Save vectorizer\n        vectorizer_path = os.path.join(model_dir, 'large_scale_vectorizer.pkl')\n        self.preprocessor.save_vectorizer(vectorizer_path)\n        \n        # Save best model\n        if self.classifier.best_model_name:\n            best_model_path = os.path.join(model_dir, f'best_model_large_scale_{self.classifier.best_model_name}.pkl')\n            self.classifier.save_model(self.classifier.best_model_name, best_model_path)\n        \n        # Save all models\n        for model_name in self.classifier.trained_models.keys():\n            model_path = os.path.join(model_dir, f'{model_name}_large_scale.pkl')\n            self.classifier.save_model(model_name, model_path)\n        \n        # Save training results\n        results_data = []\n        for model_name, results in self.training_history.items():\n            results_data.append({\n                'Model': model_name,\n                'Train_Accuracy': results.get('train_accuracy', 0),\n                'CV_Mean': results.get('cv_mean', 0),\n                'CV_Std': results.get('cv_std', 0),\n                'Test_Accuracy': results.get('test_accuracy', 0),\n                'AUC_Score': results.get('auc_score', None)\n            })\n        \n        results_df = pd.DataFrame(results_data)\n        results_path = os.path.join(model_dir, 'large_scale_training_results.csv')\n        results_df.to_csv(results_path, index=False)\n        \n        # Save configuration\n        config_data = {\n            'timestamp': datetime.now().isoformat(),\n            'max_features': self.max_features,\n            'batch_size': self.batch_size,\n            'use_lemmatization': self.use_lemmatization,\n            'best_model': self.classifier.best_model_name,\n            'vocabulary_size': len(self.preprocessor.vectorizer.vocabulary_) if self.preprocessor.vectorizer else 0\n        }\n        \n        config_path = os.path.join(model_dir, 'training_config.json')\n        import json\n        with open(config_path, 'w') as f:\n            json.dump(config_data, f, indent=2)\n        \n        print(f\"   \u2705 All models and results saved successfully!\")\n        \n        # Print summary\n        print(f\"\\n\ud83d\udcc4 Training Summary:\")\n        print(f\"   Best Model: {self.classifier.best_model_name}\")\n        print(f\"   Vocabulary Size: {config_data['vocabulary_size']:,} features\")\n        print(f\"   Models Trained: {len(self.classifier.trained_models)}\")\n        print(f\"   Results saved to: {results_path}\")\n    \n    def run_full_training(self, data_path, sample_size=None, model_dir='models_large_scale'):\n        \"\"\"\n        Run the complete training pipeline.\n        \n        Args:\n            data_path (str): Path to dataset\n            sample_size (int): Optional sample size\n            model_dir (str): Directory to save results\n        \"\"\"\n        start_time = datetime.now()\n        \n        print(\"\ud83d\ude80\" * 20)\n        print(\"  LARGE-SCALE FAKE NEWS DETECTOR TRAINING\")\n        print(\"\ud83d\ude80\" * 20)\n        print(f\"Start time: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n        \n        try:\n            # Step 1: Load and prepare data\n            X_train, X_test, y_train, y_test = self.load_and_prepare_data(data_path, sample_size)\n            \n            # Step 2: Vectorization\n            X_train_tfidf, X_test_tfidf = self.fit_vectorizer_and_transform(X_train, X_test)\n            \n            # Step 3: Model training\n            results = self.train_models(X_train_tfidf, y_train, X_test_tfidf, y_test)\n            \n            # Step 4: Save everything\n            self.save_models_and_results(model_dir)\n            \n            # Final summary\n            end_time = datetime.now()\n            duration = end_time - start_time\n            \n            print(f\"\\n\ud83c\udf86\" * 20)\n            print(\"  TRAINING COMPLETED SUCCESSFULLY!\")\n            print(f\"\ud83c\udf86\" * 20)\n            print(f\"Duration: {duration}\")\n            print(f\"Best Model: {self.classifier.best_model_name}\")\n            print(f\"Best CV Score: {max([r.get('cv_mean', 0) for r in results.values()]):.4f}\")\n            \n            return results\n            \n        except Exception as e:\n            print(f\"\\n\u274c Training failed: {e}\")\n            import traceback\n            traceback.print_exc()\n            return None

def main():\n    \"\"\"Main execution function.\"\"\"\n    parser = argparse.ArgumentParser(\n        description='Large-Scale Fake News Detector Training',\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nExamples:\n  python train_large_scale.py                                    # Train on full large dataset\n  python train_large_scale.py --sample 10000                     # Train on 10k sample\n  python train_large_scale.py --max-features 30000               # Use 30k features\n  python train_large_scale.py --no-lemmatization                 # Use stemming instead\n        \"\"\"\n    )\n    \n    parser.add_argument('--data', type=str, \n                       default='data/large_scale_fake_news_dataset.csv',\n                       help='Path to dataset CSV file')\n    parser.add_argument('--sample', type=int, default=None,\n                       help='Sample size for testing (default: full dataset)')\n    parser.add_argument('--max-features', type=int, default=20000,\n                       help='Maximum number of TF-IDF features (default: 20000)')\n    parser.add_argument('--batch-size', type=int, default=1000,\n                       help='Batch size for preprocessing (default: 1000)')\n    parser.add_argument('--no-lemmatization', action='store_true',\n                       help='Use stemming instead of lemmatization')\n    parser.add_argument('--model-dir', type=str, default='models_large_scale',\n                       help='Directory to save models (default: models_large_scale)')\n    \n    args = parser.parse_args()\n    \n    # Create trainer\n    trainer = LargeScaleTrainer(\n        max_features=args.max_features,\n        batch_size=args.batch_size,\n        use_lemmatization=not args.no_lemmatization\n    )\n    \n    # Run training\n    results = trainer.run_full_training(\n        data_path=args.data,\n        sample_size=args.sample,\n        model_dir=args.model_dir\n    )\n    \n    if results:\n        print(f\"\\n\u2705 Training completed successfully!\")\n        print(f\"Models saved to: {args.model_dir}/\")\n    else:\n        print(f\"\\n\u274c Training failed!\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()